{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Exercise scraping minifigs\n", "\n", "Minifigs are the figurines included in legosets. There are many of them, and some of them have a real personality, while others are more generic. You can find all of them on the [brickset-website](https://brickset.com/browse/minifigs). Note that we won't be downloading all the images, as that would stress the bandwidth of this free website way to much. The goal therefore is to create a list of URL's, divided by theme.\n", "\n", "If you want you can still download the image files later on, print them all individually on A4 pages and redecorate your room.\n", "\n", "But back to code. First step is including the libraries."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# ! pip install requests\n", "# ! pip install beautifulsoup4\n", "\n", "import requests\n", "from bs4 import BeautifulSoup"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Step 1\n", " \n", "Request the content of the page https://brickset.com/browse/minifigs.  \n", "Show the first and last 100 characters."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Up to you!\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Step 2\n", " \n", "So we can see we have the basic site content now. The actual site is a page with links to all the different themes, and we need all these links to go and fetch the page behind it. We reach back to trusty old Chrome Inspect to see what part of the website we're interested in:\n", "\n", "![](images/2022-03-07-21-23-48.png)\n", "\n", "We can skip everything not in ```\"<div class=\"content\">\"```.  \n", "\n", "Write the code to create a list of tuples.  Each tuple contains the category name and the url of the page showing the corresponding minifigs.\n", "\n", "![](images/minifig_part2.PNG)\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Up to you!\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Step 3\n", "\n", "We also need to store the base URL in a variable (https://brickset.com), since it's not in the href-property. \n", "\n", "Note that in the created list we don't only have links to every theme, but also to every year. Every minifig in a theme is also in a year, and visa versa.\n", "\n", "![](images/minifig_part3.PNG)\n", "\n", "We don't need to download every minifig image twice! The goal of this step is filtering out all non-theme items (~years) in the list. \n", "\n", "Try to use regex to check whether the first element of the tuple contains a year (4 digits).  \n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Up to you!\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Check your code.  Probably you didn't think of using list comprehension. In that case rewrite your solution.  "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Up to you!\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Step 4\n", "\n", "Next we'll be doing something different than before: in stead of simply putting a URL in a variable and using that for the request, we'll write a function taking the URL as parameter. This way it's easier to reuse this function later on for all the urls in the list of categories. \n", "\n", "\n", "The downside of this when using a Jupyter notebook is if the function is in codeblock A and you call it in codeblock B, then running codeblock B *won't* recompile the function. You'll run the function as it was the last time you ran codeblock A.\n", "\n", "And what does this function do? We'll be looking at the following pages:\n", "\n", "![](images/2022-03-08-16-09-11.png)\n", "\n", "As you can see the interesting part is in the section ```<section class=\"setlist minifiglist\">```. All individual images are conveniently grouped in articles with class \"set\":\n", "\n", "![](images/2022-03-08-16-17-53.png)\n", "\n", "Goal of this step is to define a function _**download_page(url)**_ that scrapes this information and stores it in a list of tuples.  The first element in the tuple is the link to the image file and the second element is the content of the attribute 'title'.\n", "\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Up to you!\n", "\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# USE THIS CODE TO TEST YOUR FUNCTION\n", "images = download_page(\"https://brickset.com/minifigs/category-Adventurers\")\n", "print(*images[0:10],sep='\\n')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The output should look like this\n", "\n", "![](images/minifig_part4.PNG)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Step 5\n", "\n", "From here we give you the code. \n", "\n", "That much is working. But there's another problem:\n", "\n", "![](images/2022-03-08-16-32-19.png)\n", "\n", "Pagination. We're not looking at all minifigs, but only the ones that fitted on the page. There are two solutions:\n", "\n", "- Do some extra scraping, and get a list of all pages, calling the function on all these pages\n", "- Make the existing function recursive: if there is a \"next page\" link, get the list from that page and add it to the returned list.\n", "\n", "The second is the topic of the first chapter of the AI-course you'll be getting. Let's do a preview!"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def download_page_recursive(url):\n", "    \n", "    page = requests.get(url)\n", "    information = []\n", "    soup = BeautifulSoup(page.content, \"html.parser\")\n", "\n", "    results = soup.find(\"section\", {\"class\": \"setlist minifiglist\"})\n", "    # print(results.prettify())\n", "    image_list = []\n", "\n", "    articles = results.find_all(\"article\", {\"class\": \"set\"})\n", "\n", "    for article in articles:\n", "        image = article.find(\"img\", src=True)\n", "        image_list.append( ( image[\"src\"], image[\"title\"] ) )\n", "\n", "    # the new part:\n", "    results = soup.find(\"li\", {\"class\": \"next\"}) # look in the entire page, not just the center part\n", "    if results != None:\n", "        link = results.find(\"a\", href=True)\n", "        if link != None:\n", "            image_list += download_page(link['href']) # add all returned links to the list we already had\n", "        \n", "    return image_list\n", "\n", "images = download_page(\"https://brickset.com/minifigs/category-Adventurers\")\n", "print(len(images))\n", "\n", "images = download_page_recursive(\"https://brickset.com/minifigs/category-Adventurers\")\n", "print(len(images))\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Five more, which checks out, because there are five minifigs on the second page. But what, so I hear you think, happens if there is a third page? Well, the second page will have a \"Next\" link as well, so the second page will ask the third page for a list, add that list to the list the second page made and return it to the function creating the list of the first page. And a fourth page? Let the third page handle that. Do note that this only works when the last page doesn't have a \"Next\"-link. If the last page were to have a link to the first page (circular pagination, so to speak) we'd have ourselves an infinite loop.\n", "\n", "Recursion is complicated, but it does great things. Just watch [this](https://www.youtube.com/watch?v=G_UYXzGuqvM) video. It won't, however, be in the exam for this course.\n", "\n", "Next up is running this recursive function on all the links we created earlier (in the code sample this variable is called 'pretty_links' You must adjust the name!). Do remember to add the base_url variable (adjust the name) to the url in the list. "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["all_images = []\n", "for pretty_link in pretty_links[0:3]:\n", "    images = download_page_recursive(base_url + pretty_link[1])\n", "    all_images += [ (pretty_link[0], im[0], im[1]) for im in images]\n", "\n", "print(all_images[:10])\n", "print(all_images[-10:])\n", "print(len(all_images))\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Did you note the \"[0:3]\" at the end of the for-loop? That is there for testing purposes. A loop like this never works on the first try and this way you can test it without always running it on the full list of a couple of thousand images. And we left it here because for this example the 119 links we have are plenty. There is no need to run all categories and download a list of 13.000 links...\n", "\n", "So now we have the list. Maybe we want it in a CSV-file? That would be a good way of storing it for later usage."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import csv\n", "\n", "header = ['Category', 'URL', 'name']\n", "\n", "with open('to_download.csv', 'w', encoding='UTF8', newline='') as f:\n", "    writer = csv.writer(f)\n", "    writer.writerow(header)\n", "    writer.writerows(all_images)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now all we need to do in the Jupyter notebook where we're downloading the images is read the csv (using CSV-reader) and recreate the exact same list-variable we had before. Which seems like a waste, isn't there a way to store the variable as a file and simply re-import the file everytime we need the file? Like we do with vegetables: we put them in a jar with a mixture of salt and vinegar to keep them until the dark days in winter when we need a homemade Bicky-burger with a homemade pickle.\n", "\n", "(The keyword here is [pickle](https://docs.python.org/3/library/pickle.html).)\n", "\n", "([And another link that is much easier to understand](https://dodona.ugent.be/nl/activities/58032010/).)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import pickle\n", "\n", "pickle.dump( all_images, open( \"all_images.p\", \"wb\" ) )"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Opening a jar with condiments can be very hard. Is the same true for opening a pickle-file?"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["my_images = pickle.load( open( \"all_images.p\", \"rb\" ) )\n", "\n", "print(my_images[:10])\n", "print(my_images[-10:])\n", "print(len(my_images))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["No."]}], "metadata": {"kernelspec": {"display_name": "Python 3.10.1 ('venv': venv)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.11.5"}, "orig_nbformat": 4, "vscode": {"interpreter": {"hash": "a5073024fd13a4ada68fc48fe0ec6adeb4357be2c08df39710965d22c5e0b44c"}}}, "nbformat": 4, "nbformat_minor": 2}